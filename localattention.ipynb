{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb7a56-f819-4d67-a3fb-b6aa959f36f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"将表示为输入序列的图映射到隐藏向量的模块\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, use_cuda):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)  # LSTM层\n",
    "        self.use_cuda = use_cuda  # 标志，指示是否使用CUDA进行计算\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.lstm(x, hidden)  # LSTM前向传播\n",
    "        return output, hidden  # 返回LSTM输出和隐藏状态\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"可训练的初始隐藏状态\"\"\"\n",
    "        # 使用零初始化隐藏状态\n",
    "        enc_init_hx = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        enc_init_cx = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        # 如果use_cuda为True，则将张量移动到CUDA设备\n",
    "        if self.use_cuda:\n",
    "            enc_init_hx = enc_init_hx.cuda()\n",
    "            enc_init_cx = enc_init_cx.cuda()\n",
    "        return (enc_init_hx, enc_init_cx)  # 返回初始化的隐藏状态作为元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf329d-5a25-4b79-8c5c-fbbd402445e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalAttention(nn.Module):\n",
    "    \"\"\"用于seq2seq解码器的通用局部注意力模块\"\"\"\n",
    "    def __init__(self, dim, window_size, use_tanh=False, C=10, use_cuda=True):\n",
    "        super(LocalAttention, self).__init__()\n",
    "        self.use_tanh = use_tanh  # 是否使用tanh函数\n",
    "        self.project_query = nn.Linear(dim, dim)  # 将查询向量投影到新的维度空间\n",
    "        self.project_ref = nn.Conv1d(dim, dim, 1, 1)  # 将参考向量投影到新的维度空间\n",
    "        self.C = C  # tanh探索\n",
    "        self.tanh = nn.Tanh()  # tanh激活函数\n",
    "        self.window_size = window_size  # 局部窗口大小\n",
    "        \n",
    "        v = torch.FloatTensor(dim)  # 创建一个张量v\n",
    "        if use_cuda:\n",
    "            v = v.cuda()  # 如果使用CUDA，将张量移动到CUDA设备\n",
    "        self.v = nn.Parameter(v)  # 将张量v包装成可学习的参数\n",
    "        self.v.data.uniform_(-(1. / math.sqrt(dim)), 1. / math.sqrt(dim))  # 对参数进行均匀初始化\n",
    "        \n",
    "    def forward(self, query, ref):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            query: 当前时间步解码器的隐藏状态。大小为 batch x dim\n",
    "            ref: 编码器的一组隐藏状态。大小为 sourceL x batch x hidden_dim\n",
    "        \"\"\"\n",
    "        # ref 现在是 [batch_size x hidden_dim x sourceL] 的张量\n",
    "        ref = ref.permute(1, 2, 0)  # 调整张量维度顺序\n",
    "        q = self.project_query(query).unsqueeze(2)  # batch x dim x 1\n",
    "        e = self.project_ref(ref)  # batch_size x hidden_dim x sourceL \n",
    "        \n",
    "        # 确定窗口边界，注意力集中在中心向左延申一半到窗口内小于sourceL的长度\n",
    "        start = max(0, i - self.window_size // 2)\n",
    "        end = min(sourceL, start + self.window_size)\n",
    "        \n",
    "        # 将查询张量扩展到 window_size，为了可以与ref逐元素操作\n",
    "        expanded_q = q.repeat(1, 1, end - start)  # batch x dim x window_size\n",
    "        \n",
    "        # 瞥见函数计算注意力权重\n",
    "        u = torch.bmm(self.v.unsqueeze(0).expand(expanded_q.size(0), -1, -1),\n",
    "                      self.tanh(expanded_q + e[:, :, start:end]))\n",
    "        if self.use_tanh:\n",
    "            logits = self.C * self.tanh(u.squeeze(1))\n",
    "        else:\n",
    "            logits = u.squeeze(1)\n",
    "            \n",
    "        return e, logits  # 返回注意力权重和logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efb031-d1e6-4f76-81cf-04d383a97ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 max_length,\n",
    "                 tanh_exploration,\n",
    "                 terminating_symbol,\n",
    "                 use_tanh,\n",
    "                 decode_type,\n",
    "                 n_glimpses=1,\n",
    "                 beam_size=0,\n",
    "                 use_cuda=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_glimpses = n_glimpses\n",
    "        self.max_length = max_length\n",
    "        self.terminating_symbol = terminating_symbol \n",
    "        self.decode_type = decode_type\n",
    "        self.beam_size = beam_size\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # 定义输入权重和隐藏权重的线性变换\n",
    "        self.input_weights = nn.Linear(embedding_dim, 4 * hidden_dim)\n",
    "        self.hidden_weights = nn.Linear(hidden_dim, 4 * hidden_dim)\n",
    "\n",
    "        # 定义注意力机制\n",
    "        self.pointer = LocalAttention(hidden_dim, use_tanh=use_tanh, C=tanh_exploration, use_cuda=self.use_cuda)\n",
    "        self.glimpse = LocalAttention(hidden_dim, use_tanh=False, use_cuda=self.use_cuda)\n",
    "        self.sm = nn.Softmax()\n",
    "\n",
    "    # 生成掩码\n",
    "    def apply_mask_to_logits(self, step, logits, mask, prev_idxs):\n",
    "        # 如果没有提供掩码，则创建一个形状与logits相同的全零张量作为掩码\n",
    "        if mask is None:\n",
    "            mask = torch.zeros(logits.size()).byte()\n",
    "            if self.use_cuda:\n",
    "                mask = mask.cuda()\n",
    "\n",
    "        # 克隆掩码，以便修改不影响原始掩码\n",
    "        mask_clone = mask.clone()\n",
    "\n",
    "        # 防止已经选择的索引再次被选择，或者允许重新选择并在目标函数中进行惩罚\n",
    "        if prev_idxs is not None:\n",
    "            # 将最近选择的符号的索引位置设为1（标记为已选择）\n",
    "            # prev_idxs 是先前已经选择的符号的索引\n",
    "            # 在掩码中标记已选择的位置，避免它们再次被选择\n",
    "            mask_clone[[x for x in range(logits.size(0))], prev_idxs.data] = 1\n",
    "\n",
    "            # 将已经选择的位置在logits中置为负无穷\n",
    "            # 这样可以确保已选择的符号不会再次被选择\n",
    "            logits[mask_clone] = -np.inf\n",
    "\n",
    "        # 返回修改后的logits和掩码\n",
    "        return logits, mask_clone\n",
    "\n",
    "\n",
    "    def forward(self, decoder_input, embedded_inputs, hidden, context):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            decoder_input: 解码器的初始输入，大小为 [batch_size x embedding_dim]，可训练参数。\n",
    "            embedded_inputs: 编码器输出的嵌入，大小为 [sourceL x batch_size x embedding_dim]\n",
    "            hidden: 前一个隐藏状态，大小为 [batch_size x hidden_dim]，初始设置为 (enc_h[-1], enc_c[-1])\n",
    "            context: 编码器输出，大小为 [sourceL x batch_size x hidden_dim] \n",
    "        \"\"\"\n",
    "        def recurrence(x, hidden, logit_mask, prev_idxs, step):\n",
    "            \n",
    "            hx, cx = hidden  # batch_size x hidden_dim\n",
    "            \n",
    "            # 计算门控信息\n",
    "            gates = self.input_weights(x) + self.hidden_weights(hx)\n",
    "            ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "            \n",
    "            ingate = F.sigmoid(ingate)\n",
    "            forgetgate = F.sigmoid(forgetgate)\n",
    "            cellgate = F.tanh(cellgate)\n",
    "            outgate = F.sigmoid(outgate)\n",
    "            # 当前状态\n",
    "            cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "            # 当前隐藏状态\n",
    "            hy = outgate * F.tanh(cy)  # batch_size x hidden_dim\n",
    "\n",
    "            #输出隐藏状态\n",
    "            g_l = hy\n",
    "            for i in range(self.n_glimpses):\n",
    "                #获取ref和对应的logit\n",
    "                ref, logits = self.glimpse(g_l, context)\n",
    "                #应用掩码，确保没有被选中\n",
    "                logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n",
    "                # [batch_size x h_dim x sourceL] * [batch_size x sourceL x 1] = \n",
    "                # [batch_size x h_dim x 1]\n",
    "                #计算加权和，获得注意力加权后的ref，更新g_l，维度转换\n",
    "                g_l = torch.bmm(ref, self.sm(logits).unsqueeze(2)).squeeze(2) \n",
    "                \n",
    "            # 用指针把g_l和context（上下文）\n",
    "            _, logits = self.pointer(g_l, context)\n",
    "            \n",
    "            # 在后续的循环中继续使用这些更新后的 logits 和掩码\n",
    "            logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n",
    "            # 用softmax来看logit的概率分布\n",
    "            probs = self.sm(logits)\n",
    "            return hy, cy, probs, logit_mask\n",
    "\n",
    "        \n",
    "        batch_size = context.size(1)\n",
    "        outputs = []\n",
    "        selections = []\n",
    "        steps = range(self.max_length)  # 或者直到终止符号？\n",
    "        inps = []\n",
    "        idxs = None\n",
    "        mask = None\n",
    "\n",
    "        #随机解码\n",
    "        if self.decode_type == \"stochastic\":\n",
    "            for i in steps:\n",
    "                hx, cx, probs, mask = recurrence(decoder_input, hidden, mask, idxs, i)\n",
    "                hidden = (hx, cx)\n",
    "                # 从嵌入向量embedded_inputs中根据probs选择解码器的下一个输入 [batch_size x hidden_dim]\n",
    "                decoder_input, idxs = self.decode_stochastic(\n",
    "                    probs,\n",
    "                    embedded_inputs,\n",
    "                    selections)\n",
    "                #跟踪解码器的输入\n",
    "                inps.append(decoder_input)\n",
    "                #生成输出序列，用beam搜索\n",
    "                #大于1，保留样本中最大的\n",
    "                if self.beam_size > 1:\n",
    "                    outputs.append(probs[:, 0,:])\n",
    "                else:\n",
    "                    #去除第一个维度为 1 的维度， (1, batch_size, sourceL) 转换为 (batch_size, sourceL)\n",
    "                    outputs.append(probs.squeeze(0))\n",
    "                # Check for indexing\n",
    "                selections.append(idxs)\n",
    "                 # Should be done decoding\n",
    "                \n",
    "                if len(active) == 0:\n",
    "                    break\n",
    "                decoder_input = Variable(decoder_input.data.repeat(self.beam_size, 1))\n",
    "\n",
    "            return (outputs, selections), hidden\n",
    "    #在selection中根据probs选\n",
    "    def decode_stochastic(self, probs, embedded_inputs, selections):\n",
    "        \"\"\"\n",
    "        通过选择与最大输出对应的输入来为解码器生成下一个输入\n",
    "\n",
    "        Args: \n",
    "            probs: [batch_size x sourceL]，概率分布张量\n",
    "            embedded_inputs: [sourceL x batch_size x embedding_dim]，嵌入的输入张量\n",
    "            selections: 在解码过程中先前选择的所有索引的列表\n",
    "       Returns:\n",
    "            大小为[batch_size x sourceL]的张量，包含对应于此次解码迭代中选择的[batch_size]个索引的输入的嵌入，以及相应的索引\n",
    "        \"\"\"\n",
    "        batch_size = probs.size(0)\n",
    "        # idxs 是[batch_size]的张量\n",
    "        #从idx中多项式采样（蒙特卡洛采样）\n",
    "        idxs = probs.multinomial().squeeze(1)\n",
    "\n",
    "        # 避免已经选择过的索引被再次选择\n",
    "        for old_idxs in selections:\n",
    "            # 将新的 idxs 与先前的 idxs 逐元素比较。如果有任何匹配，\n",
    "            # 则需要重新采样\n",
    "            if old_idxs.eq(idxs).data.any():\n",
    "                print(' [!] 由于竞争条件重新采样')\n",
    "                idxs = probs.multinomial().squeeze(1)\n",
    "                break\n",
    "\n",
    "        # 从嵌入的输入张量中选择索引对应的嵌入\n",
    "        sels = embedded_inputs[idxs.data, [i for i in range(batch_size)], :] \n",
    "        return sels, idxs\n",
    "\n",
    "\n",
    "    def decode_beam(self, probs, embedded_inputs, beam, batch_size, n_best, step):\n",
    "        active = []\n",
    "        for b in range(batch_size):\n",
    "            # 如果当前 beam[b] 已经完成，则跳过\n",
    "            if beam[b].done:\n",
    "                continue\n",
    "\n",
    "            # 尝试对当前 beam[b] 进行推进，如果无法推进，则将其添加到 active 列表中\n",
    "            if not beam[b].advance(probs.data[b]):\n",
    "                active += [b]\n",
    "        \n",
    "        \n",
    "        all_hyp, all_scores = [], []\n",
    "        for b in range(batch_size):\n",
    "            # 对每个 beam[b] 进行排序，得到最佳的 n_best 条路径及其对应的分数\n",
    "            scores, ks = beam[b].sort_best()\n",
    "            all_scores += [scores[:n_best]]\n",
    "            # 根据索引 ks，获取每个最佳路径的假设\n",
    "            hyps = zip(*[beam[b].get_hyp(k) for k in ks[:n_best]])\n",
    "            all_hyp += [hyps]\n",
    "        \n",
    "        # 将所有最佳假设的索引组成一个张量\n",
    "        all_idxs = Variable(torch.LongTensor([[x for x in hyp] for hyp in all_hyp]).squeeze())\n",
    "      \n",
    "        # 根据不同的维度情况选择合适的 idxs，确保最终选择的索引具有一致的形状\n",
    "        if all_idxs.dim() == 2:\n",
    "            if all_idxs.size(1) > n_best:\n",
    "                idxs = all_idxs[:,-1]  # 选择每行的最后一个索引\n",
    "            else:\n",
    "                idxs = all_idxs\n",
    "        elif all_idxs.dim() == 3:\n",
    "            idxs = all_idxs[:, -1, :]  # 选择最后一维的索引\n",
    "        else:\n",
    "            if all_idxs.size(0) > 1:\n",
    "                idxs = all_idxs[-1]  # 选择最后一个索引\n",
    "            else:\n",
    "                idxs = all_idxs\n",
    "        \n",
    "        # 如果使用 CUDA，则将 idxs 移动到 GPU 上\n",
    "        if self.use_cuda:\n",
    "            idxs = idxs.cuda()\n",
    "\n",
    "        # 根据 idxs 从 embedded_inputs 中获取对应的嵌入向量，\n",
    "        if idxs.dim() > 1:\n",
    "            x = embedded_inputs[idxs.transpose(0,1).contiguous().data, \n",
    "                    [x for x in range(batch_size)], :]\n",
    "        else:\n",
    "            x = embedded_inputs[idxs.data, [x for x in range(batch_size)], :]\n",
    "\n",
    "        # 将结果展平为二维张量，以及返回索引和活跃列表\n",
    "        return x.view(idxs.size(0) * n_best, embedded_inputs.size(2)), idxs, active\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
